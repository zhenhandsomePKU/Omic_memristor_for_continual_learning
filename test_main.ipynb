{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674c922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from models_utils import * \n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2403b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self,scenario='task',net='bnn',in_size=784,hidden_layers=[500,200],out_size=10,task_sequence=['KMNIST','FMNIST','MNIST'],\n",
    "                lr=0.005,gamma=1,epochs_per_task=20,norm='bn',meta=[3],rnd_consolidation=False,ewc_lambda=0,ewc=False,si_lambda=0,si=False,bin_path=False,decay=1e-7,init='uniform',init_width=0.1,\n",
    "                save=True,interleaved=False,beaker=False,fb=5e-3,n_bk=4,ratios=[1e-2,1e-3,1e-4,1e-5],areas=[1,2,4,8],\n",
    "                device=0,seed=None,bit_num=3,upper_bound=1,noise_std=0.05,line_resistance=0.01,line_ratio=1):\n",
    "        self.scenario=scenario\n",
    "        self.net=net\n",
    "        self.in_size=in_size\n",
    "        self.hidden_layers=hidden_layers\n",
    "        self.out_size=out_size\n",
    "        self.task_sequence=task_sequence\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.epochs_per_task=epochs_per_task\n",
    "        self.norm=norm\n",
    "        self.meta=meta\n",
    "        self.rnd_consolidation=rnd_consolidation\n",
    "        self.ewc_lambda=ewc_lambda\n",
    "        self.ewc=ewc\n",
    "        self.si_lambda=si_lambda\n",
    "        self.si=si\n",
    "        self.bin_path=bin_path\n",
    "        self.decay=decay\n",
    "        self.init=init\n",
    "        self.init_width=init_width\n",
    "        self.save=save\n",
    "        self.interleaved=interleaved\n",
    "        self.beaker=beaker\n",
    "        self.fb=fb\n",
    "        self.n_bk=n_bk\n",
    "        self.ratios=ratios\n",
    "        self.areas=areas\n",
    "        self.device=device\n",
    "        self.seed=seed\n",
    "        self.bit_num=bit_num\n",
    "        self.upper_bound=upper_bound\n",
    "        self.noise_std=noise_std\n",
    "        \n",
    "        \n",
    "def set_line_res_matrix(row_num, column_num, res_unit, ratio ):\n",
    "    # res_unit: after normalization\n",
    "    line_resistance_matrix=torch.zeros(row_num, column_num)\n",
    "    for row in range(row_num):\n",
    "        for column in range(column_num):\n",
    "            line_resistance_matrix[row][column] = (row*ratio+ratio+2*(column+1)*(1/ratio))*res_unit  \n",
    "    return line_resistance_matrix\n",
    "\n",
    "def generate_failure_matrix(fail_num,len_1,len_2,previous_weight_matrix):\n",
    "        weight_matrix = previous_weight_matrix\n",
    "        \n",
    "        count=0\n",
    "        for try_num in range(1000000):\n",
    "            if count > fail_num:\n",
    "                break\n",
    "            row = np.random.randint(0,len_1)\n",
    "            column = np.random.randint(0,len_2)\n",
    "            \n",
    "            if int(weight_matrix[row][column]) == 5:\n",
    "                count+=1\n",
    "                weight_matrix[row][column]=np.random.randint(0,3)-1 \n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "        #weight_matrix.add_(torch.normal(mean=0.0, std=0.1457, size=weight_matrix.shape))\n",
    "\n",
    "        return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the sequence： ['FMNIST','MNIST','KMNIST'],['FMNIST','KMNIST','MNIST'],['MNIST','FMNIST','KMNIST'],['MNIST','KMNIST','FMNIST'],['KMNIST','MNIST','FMNIST'] ['KMNIST','FMNIST','MNIST']\n",
    "\n",
    "#for test_time in range(1):\n",
    "    #for tasks_test in [['FMNIST','MNIST','KMNIST'],['KMNIST','FMNIST','MNIST']]:\n",
    "#choose Bit=3 upper_bound=1.5 作为 探究 noise_std\n",
    "#for bit_num_test in [1,2,3,4,5]:\n",
    "\n",
    "#for line_resistance_test in [0.01,0.02,0.05,0.08,0.1,0.12,0.15,0.2,0.25,0.28]:\n",
    "    #for line_ratio_test in [0.1,0.25,0.5,1,1.25,1.5,2,2.5,4,5]:\n",
    "    \n",
    "    \n",
    "    \n",
    "for epochs_per_task_test in [20]:\n",
    "    \n",
    "    \n",
    "    fail_ratio=0.0001\n",
    "    failure_weight_matrix_list={}\n",
    "\n",
    "    for cycle_num in tqdm(range(epochs_per_task_test*3)):\n",
    "\n",
    "        failure_weight_matrix_list[cycle_num]={}\n",
    "\n",
    "        if cycle_num == 0:\n",
    "            previous_matrix_0=torch.full((500,784),5)\n",
    "            previous_matrix_1=torch.full((200,500),5)\n",
    "            previous_matrix_2=torch.full((10,200),5)\n",
    "            failure_weight_matrix_list[cycle_num][0]=generate_failure_matrix(fail_ratio*500*784,500,784,previous_matrix_0)\n",
    "            failure_weight_matrix_list[cycle_num][1]=generate_failure_matrix(fail_ratio*200*500,200,500,previous_matrix_1)\n",
    "            failure_weight_matrix_list[cycle_num][2]=generate_failure_matrix(fail_ratio*10*200,10,200,previous_matrix_2)\n",
    "\n",
    "        else:\n",
    "            previous_matrix_0=failure_weight_matrix_list[cycle_num-1][0]\n",
    "            previous_matrix_1=failure_weight_matrix_list[cycle_num-1][1]\n",
    "            previous_matrix_2=failure_weight_matrix_list[cycle_num-1][2]\n",
    "            failure_weight_matrix_list[cycle_num][0]=generate_failure_matrix(fail_ratio*500*784,500,784,previous_matrix_0)\n",
    "            failure_weight_matrix_list[cycle_num][1]=generate_failure_matrix(fail_ratio*200*500,200,500,previous_matrix_1)\n",
    "            failure_weight_matrix_list[cycle_num][2]=generate_failure_matrix(fail_ratio*10*200,10,200,previous_matrix_2)\n",
    "\n",
    "        #print(failure_weight_matrix_list[cycle_num][2][:1,:])\n",
    "\n",
    "        #设置线阻矩阵\n",
    "        line_resistance_test=0.02\n",
    "        line_ratio_test=1\n",
    "        line_resistance_matrix_0 = set_line_res_matrix(500,784,line_resistance_test/3652,line_ratio_test)\n",
    "        line_resistance_matrix_1 = set_line_res_matrix(200,500,line_resistance_test/3652,line_ratio_test)\n",
    "        line_resistance_matrix_2 = set_line_res_matrix(10,200,line_resistance_test/3652,line_ratio_test)\n",
    "\n",
    "        line_res_matrix=[line_resistance_matrix_0,line_resistance_matrix_1,line_resistance_matrix_2]\n",
    "\n",
    "\n",
    "        args=Args(meta=[3],bit_num=3,upper_bound=1.5,noise_std=0.05,epochs_per_task=epochs_per_task_test,line_resistance=line_resistance_test,line_ratio=line_ratio_test)\n",
    "        device = torch.device(\"cuda:\"+str(args.device) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if args.seed is not None:\n",
    "            torch.manual_seed(args.seed)\n",
    "            np.random.seed(args.seed)\n",
    "\n",
    "        date = datetime.now().strftime('%Y-%m-%d')\n",
    "        time = datetime.now().strftime('%H-%M-%S')\n",
    "        path = r'G:\\Germany_project\\Response_file\\neural_network'\n",
    "        if not(os.path.exists(path)):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        createHyperparametersFile(path, args)\n",
    "\n",
    "        train_loader_list = []\n",
    "        test_loader_list = []\n",
    "        dset_train_list = []\n",
    "        task_names = []\n",
    "\n",
    "        '''\n",
    "        def shuffle(x):\n",
    "            x=x.view(-1)\n",
    "            permut = torch.from_numpy(np.random.permutation(784))\n",
    "            a=[]\n",
    "            for i in range(len(permut)):\n",
    "                a.append(x[permut[i]])\n",
    "            a=torch.tensor(a)\n",
    "            a=a.view(1,28,28)\n",
    "            return a\n",
    "        #torchvision.transforms.Lambda(shuffle),\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        for idx, task in enumerate(args.task_sequence):\n",
    "            if task == 'MNIST':\n",
    "                train_loader_list.append(mnist_train_loader)\n",
    "                test_loader_list.append(mnist_test_loader)\n",
    "                dset_train_list.append(mnist_dset_train)\n",
    "                task_names.append(task)\n",
    "            elif task == 'USPS':\n",
    "                train_loader_list.append(usps_train_loader)\n",
    "                test_loader_list.append(usps_test_loader)\n",
    "                dset_train_list.append(usps_dset_train)\n",
    "                task_names.append(task)\n",
    "            elif task == 'CMNIST':\n",
    "                train_loader_list.append(cmnist_train_loader)\n",
    "                test_loader_list.append(cmnist_test_loader)\n",
    "                dset_train_list.append(cmnist_dset_train)\n",
    "                task_names.append(task)\n",
    "            elif task == 'FMNIST':\n",
    "                train_loader_list.append(fashion_mnist_train_loader)\n",
    "                test_loader_list.append(fashion_mnist_test_loader)\n",
    "                dset_train_list.append(fmnist_dset_train)\n",
    "                task_names.append(task)\n",
    "\n",
    "            elif task == 'KMNIST':\n",
    "                train_loader_list.append(kmnist_train_loader)\n",
    "                test_loader_list.append(kmnist_test_loader)\n",
    "                dset_train_list.append(kmnist_dset_train)\n",
    "                task_names.append(task)\n",
    "\n",
    "            elif task == 'KaMNIST':\n",
    "                train_loader_list.append(kamnist_train_loader)\n",
    "                test_loader_list.append(kamnist_test_loader)\n",
    "                dset_train_list.append(kamnist_dset_train)\n",
    "                task_names.append(task)\n",
    "\n",
    "            elif task == 'pMNIST':\n",
    "\n",
    "                transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                           torchvision.transforms.Lambda(shuffle),\n",
    "                              torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,))])\n",
    "\n",
    "                dset_train = torchvision.datasets.MNIST('./mnist_pytorch', train=True, transform=transform, target_transform=None, download=True)\n",
    "                train_loader = torch.utils.data.DataLoader(dset_train, batch_size=100, shuffle=True,num_workers=0)\n",
    "\n",
    "                dset_test = torchvision.datasets.MNIST('./mnist_pytorch', train=False, transform=transform, target_transform=None, download=True)\n",
    "                test_loader = torch.utils.data.DataLoader(dset_test, batch_size=100, shuffle=False,num_workers=0)\n",
    "                #train_loader, test_loader, dset_train = create_permuted_loaders('MNIST')\n",
    "                train_loader_list.append(train_loader)\n",
    "                test_loader_list.append(test_loader)\n",
    "                dset_train_list.append(dset_train)\n",
    "                task_names.append(task+str(idx+1))\n",
    "\n",
    "            elif task == 'animals':\n",
    "                animals_train_loader, animals_test_loader, animals_dset_train = process_cifar10(task)\n",
    "                train_loader_list.append(animals_train_loader)\n",
    "                test_loader_list.append(animals_test_loader)\n",
    "                dset_train_list.append(animals_dset_train)\n",
    "                task_names.append('animals')\n",
    "            elif task == 'vehicles':\n",
    "                vehicles_train_loader, vehicles_test_loader, vehicles_dset_train = process_cifar10(task)\n",
    "                train_loader_list.append(vehicles_train_loader)\n",
    "                test_loader_list.append(vehicles_test_loader)\n",
    "                dset_train_list.append(vehicles_dset_train)\n",
    "                task_names.append('vehicles')\n",
    "            elif 'cifar100' in task:\n",
    "                n_subset = int(task.split('-')[1])  # task = \"cifar100-20\" -> n_subset = 20\n",
    "                train_loader_list, test_loader_list, dset_train_list = process_cifar100(n_subset)\n",
    "                task_names = ['cifar100-'+str(i+1) for i in range(n_subset)]\n",
    "\n",
    "        if args.interleaved:\n",
    "            dset_train = torch.utils.data.ConcatDataset(dset_train_list)\n",
    "            print(len(dset_train))\n",
    "            train_loader = torch.utils.data.DataLoader(dset_train, batch_size=100, shuffle=True)\n",
    "            train_loader_list = [train_loader]\n",
    "        # Hyperparameters\n",
    "        lr = args.lr\n",
    "        epochs = args.epochs_per_task\n",
    "        save_result = args.save\n",
    "        #meta = args.meta\n",
    "        ewc_lambda = args.ewc_lambda\n",
    "        si_lambda = args.si_lambda\n",
    "        archi = [args.in_size] + args.hidden_layers + [args.out_size]\n",
    "\n",
    "        if args.net =='bnn':\n",
    "            model = BNN( archi, init = args.init, width = args.init_width, norm = args.norm).to(device)\n",
    "        elif args.net =='dnn':\n",
    "            model = DNN( archi, init = args.init, width = args.init_width).to(device)\n",
    "        elif args.net=='bcnn':\n",
    "            model = ConvBNN(init = args.init, width = args.init_width, norm=args.norm).to(device)\n",
    "\n",
    "        meta = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            index = int(n[9])\n",
    "            p.newname = 'l'+str(index)\n",
    "            if ('fc' in n) or ('cv' in n):\n",
    "                meta[p.newname] = args.meta[index-1] if len(args.meta)>1 else args.meta[0]\n",
    "\n",
    "\n",
    "\n",
    "        print(model)\n",
    "        #plot_parameters(model, path, save=save_result)\n",
    "\n",
    "        previous_tasks_parameters = {}\n",
    "        previous_tasks_fisher = {}\n",
    "\n",
    "        # ewc parameters initialization\n",
    "        if args.ewc:\n",
    "            for n, p in model.named_parameters():\n",
    "                if n.find('bn') == -1: #we dont store bn parameters as we allow task dependent bn\n",
    "                    n = n.replace('.', '__')\n",
    "                    previous_tasks_fisher[n] = []\n",
    "                    previous_tasks_parameters[n] = [] \n",
    "        elif args.si:\n",
    "            W = {}\n",
    "            p_prev = {}\n",
    "            p_old = {}\n",
    "            omega = {}\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    n = n.replace('.', '__')\n",
    "                    W[n] = p.data.clone().zero_()\n",
    "                    omega[n] = p.data.clone().zero_()\n",
    "                    if args.net=='bnn':\n",
    "                        p_prev[n] = p.data.clone()  # or sign\n",
    "                        if args.bin_path:\n",
    "                            p_old[n] = p.data.sign().clone()\n",
    "                        else:\n",
    "                            p_old[n] = p.data.clone()\n",
    "                    elif args.net=='dnn':\n",
    "                        p_prev[n] = p.data.clone()\n",
    "                        p_old[n] = p.data.clone()\n",
    "        data = {}\n",
    "        data['net'] = args.net\n",
    "        data['scenario'] = args.scenario\n",
    "        arch = ''\n",
    "        if not(args.net=='bcnn'):\n",
    "            for i in range(model.hidden_layers):\n",
    "               arch = arch + '-' + str(model.layers_dims[i+1])\n",
    "\n",
    "        data['arch'] = arch[1:]\n",
    "        data['norm'] = args.norm\n",
    "        data['lr'], data['meta'], data['ewc'], data['SI'], data['task_order'] = [], [], [], [], []  \n",
    "        data['tsk'], data['epoch'], data['acc_tr'], data['loss_tr'] = [], [], [], []\n",
    "        for i in range(len(test_loader_list)):\n",
    "            data['acc_test_tsk_'+str(i+1)], data['loss_test_tsk_'+str(i+1)] = [], []\n",
    "\n",
    "        name = '_'+data['net']+'_'+data['arch']+'_'\n",
    "        for t in range(len(task_names)):\n",
    "            if ('cifar100' in task_names[t]) and ('cifar100' in name):\n",
    "                pass\n",
    "            else:\n",
    "                name = name+task_names[t]+'-'\n",
    "\n",
    "        bn_states=[]        \n",
    "        lrs = [lr*(args.gamma**(-i)) for i in range(len(train_loader_list))] \n",
    "\n",
    "        if args.beaker:\n",
    "            optimizer = Adam_bk(model.parameters(), lr = lr, n_bk=args.n_bk, ratios=args.ratios, areas=args.areas, feedback=args.fb, meta=meta, weight_decay=args.decay, path=path)\n",
    "        if args.si:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = args.decay)\n",
    "\n",
    "        for task_idx, task in enumerate(train_loader_list):\n",
    "            if not(args.beaker or args.si):\n",
    "                optimizer = Adam_meta(model.parameters(), lr = lrs[task_idx], meta = meta, weight_decay = args.decay)\n",
    "\n",
    "\n",
    "\n",
    "        #进行计算的过程\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "            print('No.epoch:{}'.format((task_idx)*epochs+epoch))\n",
    "\n",
    "            if args.ewc:\n",
    "                train(model, task, task_idx, optimizer, device, args, prev_cons=previous_tasks_fisher, \n",
    "                        prev_params=previous_tasks_parameters) \n",
    "            elif args.si:\n",
    "                train(model, task, task_idx, optimizer, device, args, prev_cons=omega, path_integ=W, prev_params=(p_prev, p_old) ) \n",
    "            else:\n",
    "                train(model, task, task_idx, optimizer, device, args)\n",
    "\n",
    "            data['task_order'].append(task_idx+1)\n",
    "            data['tsk'].append(task_names[task_idx])\n",
    "            data['epoch'].append(epoch)\n",
    "            data['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            train_accuracy, train_loss = test(model, task, device, verbose=True, line_res_matrix=line_res_matrix,fail_matrix=failure_weight_matrix_list[(task_idx)*epochs+epoch-1])\n",
    "\n",
    "            data['acc_tr'].append(train_accuracy)\n",
    "            data['loss_tr'].append(train_loss)\n",
    "            data['meta'].append(meta)\n",
    "            data['ewc'].append(ewc_lambda)\n",
    "            data['SI'].append(si_lambda)\n",
    "\n",
    "            current_bn_state = model.save_bn_states()\n",
    "\n",
    "            for other_task_idx, other_task in enumerate(test_loader_list):\n",
    "\n",
    "                if args.scenario == 'task':\n",
    "                    if other_task_idx>=task_idx:\n",
    "                        model.load_bn_states(current_bn_state)\n",
    "                        test_accuracy, test_loss = test(model , other_task, device, verbose=(other_task_idx==task_idx),line_res_matrix=line_res_matrix,\n",
    "                                                        fail_matrix=failure_weight_matrix_list[(task_idx)*epochs+epoch-1])\n",
    "                    else:\n",
    "                        model.load_bn_states(bn_states[other_task_idx])\n",
    "                        test_accuracy, test_loss = test(model , other_task, device,line_res_matrix=line_res_matrix, \n",
    "                                                        fail_matrix=failure_weight_matrix_list[(task_idx)*epochs+epoch-1])\n",
    "\n",
    "                elif args.scenario =='domain':\n",
    "                    test_accuracy, test_loss = test(model, other_task, device, verbose=True,line_res_matrix=line_res_matrix, \n",
    "                                                    fail_matrix=failure_weight_matrix_list[(task_idx)*epochs+epoch-1])\n",
    "\n",
    "                data['acc_test_tsk_'+str(other_task_idx+1)].append(test_accuracy)\n",
    "                print(data['acc_test_tsk_'+str(other_task_idx+1)])\n",
    "                data['loss_test_tsk_'+str(other_task_idx+1)].append(test_loss)\n",
    "\n",
    "            model.load_bn_states(current_bn_state)\n",
    "\n",
    "        plot_parameters(model, path, save=save_result)\n",
    "        # Uncomment for hidden weight histogram of Fig. 2g,h\n",
    "        #time = datetime.now().strftime('%H-%M-%S')\n",
    "        #for l in range(model.hidden_layers + 1):\n",
    "        #    torch.save(model.layers['fc'+str(l+1)].weight.org.data, path+'/'+time+'_weights_fc'+str(l+1)+'.pt')\n",
    "\n",
    "        #将已经test过的内容的BN参数进行保留\n",
    "        bn_states.append(current_bn_state)\n",
    "\n",
    "        #高级对比内容\n",
    "        if args.ewc:\n",
    "            fisher = estimate_fisher(model, dset_train_list[task_idx], device, num=5000, empirical=True)\n",
    "            for n, p in model.named_parameters():\n",
    "                if n.find('bn') == -1: # not batchnorm\n",
    "                    n = n.replace('.', '__')\n",
    "\n",
    "                    # random consolidation\n",
    "                    if args.rnd_consolidation:\n",
    "                        idx = torch.randperm(fisher[n].nelement())\n",
    "                        previous_tasks_fisher[n].append(fisher[n].view(-1)[idx].view(fisher[n].size()))\n",
    "\n",
    "                    # EWC consolidation, comment when using random consolidation\n",
    "                    previous_tasks_fisher[n].append(fisher[n])\n",
    "                    previous_tasks_parameters[n].append(p.detach().clone())\n",
    "\n",
    "        elif args.si:\n",
    "            omega = update_omega(model, omega, p_prev, W)\n",
    "            for n, p in model.named_parameters():\n",
    "                if n.find('bn') == -1: # not batchnorm\n",
    "                    n = n.replace('.','__')\n",
    "                    if args.net=='bnn':\n",
    "                        p_prev[n] = p.org.detach().clone()  # or sign\n",
    "                    else:\n",
    "                        p_prev[n] = p.detach().clone()\n",
    "\n",
    "\n",
    "\n",
    "        i=0\n",
    "\n",
    "        for (n, p) in model.named_parameters():\n",
    "\n",
    "            if (n.find('bias') == -1) and (len(p.size()) != 1):  #bias or batchnorm weight -> no plot\n",
    "                if model.__class__.__name__.find('B') != -1:  #BVGG -> plot p.org\n",
    "                    if hasattr(p,'org'):\n",
    "                        weights_org_hist = p.org.data.flatten().cpu().numpy()\n",
    "                        weights_org = p.org.data.cpu().numpy()\n",
    "                        weights_hist = p.data.flatten().cpu().numpy()\n",
    "                        weights = p.data.cpu().numpy()\n",
    "\n",
    "                        np.savetxt(path+'//'+args.task_sequence[task_idx]+'_FC_'+str(i)+'_weights_org_hist.csv',weights_org_hist,delimiter=',')\n",
    "                        np.savetxt(path+'//'+args.task_sequence[task_idx]+'_FC_'+str(i)+'_weights_org.csv',weights_org,delimiter=',')\n",
    "                        np.savetxt(path+'//'+args.task_sequence[task_idx]+'_FC_'+str(i)+'_weights_hist.csv',weights_hist,delimiter=',')\n",
    "                        np.savetxt(path+'//'+args.task_sequence[task_idx]+'_FC_'+str(i)+'_weights.csv',weights,delimiter=',')\n",
    "                        i+=1\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    weight_data= p.data.cpu().numpy()\n",
    "                    np.savetxt(path+'//'+'FC_'+str(i)+'_weights.csv',weight_data,delimiter=',')\n",
    "                    i+=1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        time = datetime.now().strftime('%H-%M-%S')\n",
    "        df_data = pd.DataFrame(data)\n",
    "        if save_result:\n",
    "            df_data.to_csv(path +'/'+time+name+'.csv', index = False)\n",
    "\n",
    "        data_write={}\n",
    "        for i_task in range(len(args.task_sequence)):\n",
    "            data_write[args.task_sequence[i_task]]=data['acc_test_tsk_'+str(i_task+1)]\n",
    "        d=pd.DataFrame(data_write)\n",
    "\n",
    "        d.to_csv(path+'/data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30404ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
